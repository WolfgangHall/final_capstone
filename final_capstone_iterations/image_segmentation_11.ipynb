{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import math\n",
    "from sklearn import mixture\n",
    "from sklearn.utils import shuffle\n",
    "from skimage import measure\n",
    "from glob import glob\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type_1\n",
      "Type_2\n",
      "Type_3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(check_output([\"ls\", \"../train\"]).decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA = \"../train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "types = ['Type_1', 'Type_2', 'Type_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../train/Type_1/579.jpg']\n",
      "['579']\n",
      "['../train/Type_2/992.jpg']\n",
      "['992']\n",
      "['../train/Type_3/835.jpg']\n",
      "['835']\n",
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 8.82 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for type in enumerate(types):\n",
    "    type_i_files = glob(os.path.join(TRAIN_DATA, type[1], \"*.jpg\"))\n",
    "    print(type_i_files[:1])\n",
    "    type_i_ids = np.array([s[len(TRAIN_DATA)+8:-4] for s in type_i_files])\n",
    "    print(type_i_ids[:1])\n",
    "    type_ids.append(type_i_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_filename(image_id, image_type):\n",
    "    \"\"\"\n",
    "    Method to grab image file path from its id and tpye\n",
    "    \"\"\"\n",
    "    if image_type == \"Type_1\" or \\\n",
    "        image_type == \"Type_2\" or \\\n",
    "        image_type == \"Type_3\":\n",
    "        data_path = os.path.join(TRAIN_DATA, image_type)\n",
    "    else:\n",
    "        raise Exception(\"Image type {} is not recognized\".format(image_type))\n",
    "        \n",
    "    ext = 'jpg'\n",
    "    return os.path.join(data_path, \"{}.{}\".format(image_id, ext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_data(image_id, image_type):\n",
    "    \"\"\"\n",
    "    Method to get image data as np.array that specifies image id and type\n",
    "    \"\"\"\n",
    "    fname = get_filename(image_id, image_type)\n",
    "    img = cv2.imread(fname)\n",
    "    assert img is not None, \"Failed to read image: {}, {}\".format(image_id, image_type)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Crop images that may have circular frames present\n",
    "# Find the largest inscribed rectangled in the thresholded image\n",
    "# image is cropped to that rectangle\n",
    "\n",
    "def maxHist(hist):\n",
    "    maxArea = (0, 0, 0)\n",
    "    height = []\n",
    "    position = []\n",
    "    for i in range(len(hist)):\n",
    "        if (len(height) == 0):\n",
    "            if (hist[i] > 0):\n",
    "                height.append(hist[i])\n",
    "                position.append(i)\n",
    "        else: \n",
    "            if (hist[i] > height[-1]):\n",
    "                height.append(hist[i])\n",
    "                position.append(i)\n",
    "            elif (hist[i] < height[-1]):\n",
    "                while (height[-1] > hist[i]):\n",
    "                    maxHeight = height.pop()\n",
    "                    area = maxHeight * (i-position[-1])\n",
    "                    if (area > maxArea[0]):\n",
    "                        maxArea = (area, position[-1], i)\n",
    "                    last_position = position.pop()\n",
    "                    if (len(height) == 0):\n",
    "                        break\n",
    "                position.append(last_position)\n",
    "                if (len(height) == 0):\n",
    "                    height.append(hist[i])\n",
    "                elif(height[-1] < hist[i]):\n",
    "                    height.append(hist[i])\n",
    "                else:\n",
    "                    position.pop()    \n",
    "    while (len(height) > 0):\n",
    "        maxHeight = height.pop()\n",
    "        last_position = position.pop()\n",
    "        area =  maxHeight * (len(hist) - last_position)\n",
    "        if (area > maxArea[0]):\n",
    "            maxArea = (area, len(hist), last_position)\n",
    "    return maxArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxRect(img):\n",
    "    maxArea = (0, 0, 0)\n",
    "    addMat = np.zeros(img.shape)\n",
    "    for r in range(img.shape[0]):\n",
    "        if r == 0:\n",
    "            addMat[r] = img[r]\n",
    "            area = maxHist(addMat[r])\n",
    "            if area[0] > maxArea[0]:\n",
    "                maxArea = area + (r,)\n",
    "        else:\n",
    "            addMat[r] = img[r] + addMat[r-1]\n",
    "            addMat[r][img[r] == 0] *= 0\n",
    "            area = maxHist(addMat[r])\n",
    "            if area[0] > maxArea[0]:\n",
    "                maxArea = area + (r,)\n",
    "    return (int(maxArea[3]+1-maxArea[0]/abs(maxArea[1]-maxArea[2])), maxArea[2], maxArea[3], maxArea[1], maxArea[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cropCircle(img):\n",
    "    if(img.shape[0] > img.shape[1]):\n",
    "        tile_size = (int(img.shape[1]*256/img.shape[0]),256)\n",
    "    else:\n",
    "        tile_size = (256, int(img.shape[0]*256/img.shape[1]))\n",
    "\n",
    "    img = cv2.resize(img, dsize=tile_size)\n",
    "            \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY);\n",
    "    _, thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    _, contours, _ = cv2.findContours(thresh.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    main_contour = sorted(contours, key = cv2.contourArea, reverse = True)[0]\n",
    "            \n",
    "    ff = np.zeros((gray.shape[0],gray.shape[1]), 'uint8') \n",
    "    cv2.drawContours(ff, main_contour, -1, 1, 15)\n",
    "    ff_mask = np.zeros((gray.shape[0]+2,gray.shape[1]+2), 'uint8')\n",
    "    cv2.floodFill(ff, ff_mask, (int(gray.shape[1]/2), int(gray.shape[0]/2)), 1)\n",
    "    \n",
    "    rect = maxRect(ff)\n",
    "    rectangle = [min(rect[0],rect[2]), max(rect[0],rect[2]), min(rect[1],rect[3]), max(rect[1],rect[3])]\n",
    "    img_crop = img[rectangle[0]:rectangle[1], rectangle[2]:rectangle[3]]\n",
    "    cv2.rectangle(ff,(min(rect[1],rect[3]),min(rect[0],rect[2])),(max(rect[1],rect[3]),max(rect[0],rect[2])),3,2)\n",
    "    \n",
    "    return [img_crop, rectangle, tile_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For initial delineation of cervix, use two features\n",
    "    # a color: the higher the a value, the redder the pixel color\n",
    "    # R: distance of a pixel from image center -> spatial information\n",
    "    #    -> support extraction of continuous regions\n",
    "\n",
    "def Ra_space(img, Ra_ratio, a_threshold):\n",
    "    imgLab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB);\n",
    "    w = img.shape[0]\n",
    "    h = img.shape[1]\n",
    "    Ra = np.zeros((w*h, 2))\n",
    "    for i in range(w):\n",
    "        for j in range(h):\n",
    "            R = math.sqrt((w/2-i)*(w/2-i) + (h/2-j)*(h/2-j))\n",
    "            Ra[i*h+j, 0] = R\n",
    "            Ra[i*h+j, 1] = min(imgLab[i][j][1], a_threshold)\n",
    "            \n",
    "    Ra[:,0] /= max(Ra[:,0])\n",
    "    Ra[:,0] *= Ra_ratio\n",
    "    Ra[:,1] /= max(Ra[:,1])\n",
    "\n",
    "    return Ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image is separated into two clusters in the 2-D (a-R) features space\n",
    "# use of Gaussian mixture modeling\n",
    "# initialized by k-means procedure\n",
    "\n",
    "def get_and_crop_image(image_id, image_type):\n",
    "    img = get_image_data(image_id, image_type)\n",
    "    initial_shape = img.shape\n",
    "    [img, rectangle_cropCircle, tile_size] = cropCircle(img)\n",
    "    imgLab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB);\n",
    "    w = img.shape[0]\n",
    "    h = img.shape[1]\n",
    "    Ra = Ra_space(imgLab, 1.0, 150)\n",
    "    a_channel = np.reshape(Ra[:,1], (w,h))\n",
    "    \n",
    "    g = mixture.GaussianMixture(n_components = 2, covariance_type = 'diag', random_state = 0, init_params = 'kmeans')\n",
    "    image_array_sample = shuffle(Ra, random_state=0)[:1000]\n",
    "    g.fit(image_array_sample)\n",
    "    labels = g.predict(Ra)\n",
    "    labels += 1 # Add 1 to avoid labeling as 0 since regionprops ignores the 0-label.\n",
    "    \n",
    "    # The cluster that has the highest a-mean is selected.\n",
    "    labels_2D = np.reshape(labels, (w,h))\n",
    "    gg_labels_regions = measure.regionprops(labels_2D, intensity_image = a_channel)\n",
    "    gg_intensity = [prop.mean_intensity for prop in gg_labels_regions]\n",
    "    cervix_cluster = gg_intensity.index(max(gg_intensity)) + 1\n",
    "\n",
    "    mask = np.zeros((w * h,1),'uint8')\n",
    "    mask[labels==cervix_cluster] = 255\n",
    "    mask_2D = np.reshape(mask, (w,h))\n",
    "\n",
    "    cc_labels = measure.label(mask_2D, background=0)\n",
    "    regions = measure.regionprops(cc_labels)\n",
    "    areas = [prop.area for prop in regions]\n",
    "\n",
    "    regions_label = [prop.label for prop in regions]\n",
    "    largestCC_label = regions_label[areas.index(max(areas))]\n",
    "    mask_largestCC = np.zeros((w,h),'uint8')\n",
    "    mask_largestCC[cc_labels==largestCC_label] = 255\n",
    "\n",
    "    img_masked = img.copy()\n",
    "    img_masked[mask_largestCC==0] = (0,0,0)\n",
    "    img_masked_gray = cv2.cvtColor(img_masked, cv2.COLOR_RGB2GRAY);\n",
    "            \n",
    "    _,thresh_mask = cv2.threshold(img_masked_gray,0,255,0)\n",
    "            \n",
    "    kernel = np.ones((11,11), np.uint8)\n",
    "    thresh_mask = cv2.dilate(thresh_mask, kernel, iterations = 1)\n",
    "    thresh_mask = cv2.erode(thresh_mask, kernel, iterations = 2)\n",
    "    _, contours_mask, _ = cv2.findContours(thresh_mask.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    main_contour = sorted(contours_mask, key = cv2.contourArea, reverse = True)[0]\n",
    "    cv2.drawContours(img, main_contour, -1, 255, 3)\n",
    "    \n",
    "    x,y,w,h = cv2.boundingRect(main_contour)\n",
    "    \n",
    "    rectangle = [x+rectangle_cropCircle[2],\n",
    "                 y+rectangle_cropCircle[0],\n",
    "                 w,\n",
    "                 h,\n",
    "                 initial_shape[0],\n",
    "                 initial_shape[1],\n",
    "                 tile_size[0],\n",
    "                 tile_size[1]]\n",
    "\n",
    "    return [image_id, img, rectangle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parallelize_image_cropping(image_ids):\n",
    "    out = open('rectangles.csv', \"w\")\n",
    "    out.write(\"image_id,type,x,y,w,h,img_shp_0_init,img_shape1_init,img_shp_0,img_shp_1\\n\")\n",
    "    imf_d = {}\n",
    "    p = Pool(cpu_count())\n",
    "    for type in enumerate(types):\n",
    "        partial_get_and_crop = partial(get_and_crop_image, image_type = type[1])    \n",
    "        ret = p.map(partial_get_and_crop, image_ids[type[0]])\n",
    "        for i in range(len(ret)):\n",
    "            out.write(image_ids[type[0]][i])\n",
    "            out.write(',' + str(type[1]))\n",
    "            out.write(',' + str(ret[i][2][0]))\n",
    "            out.write(',' + str(ret[i][2][1]))\n",
    "            out.write(',' + str(ret[i][2][2]))\n",
    "            out.write(',' + str(ret[i][2][3]))\n",
    "            out.write(',' + str(ret[i][2][4]))\n",
    "            out.write(',' + str(ret[i][2][5]))\n",
    "            out.write(',' + str(ret[i][2][6]))\n",
    "            out.write(',' + str(ret[i][2][7]))\n",
    "            out.write('\\n')\n",
    "            img = get_image_data(image_ids[type[0]][i], type[1])\n",
    "            if(img.shape[0] > img.shape[1]):\n",
    "                tile_size = (int(img.shape[1]*256/img.shape[0]), 256)\n",
    "            else:\n",
    "                tile_size = (256, int(img.shape[0]*256/img.shape[1]))\n",
    "            img = cv2.resize(img, dsize=tile_size)\n",
    "            cv2.rectangle(img,\n",
    "                          (ret[i][2][0], ret[i][2][1]), \n",
    "                          (ret[i][2][0]+ret[i][2][2], ret[i][2][1]+ret[i][2][3]),\n",
    "                          255,\n",
    "                          2)\n",
    "            #plt.imshow(img)\n",
    "            #plt.show()\n",
    "        ret = []\n",
    "    out.close()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39min 46s, sys: 1min 33s, total: 41min 20s\n",
      "Wall time: 11min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parallelize_image_cropping(type_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Segmented Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('rectangles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "      <th>img_shp_0_init</th>\n",
       "      <th>img_shape1_init</th>\n",
       "      <th>img_shp_0</th>\n",
       "      <th>img_shp_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>579</td>\n",
       "      <td>Type_1</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>190</td>\n",
       "      <td>155</td>\n",
       "      <td>4128</td>\n",
       "      <td>3096</td>\n",
       "      <td>192</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>787</td>\n",
       "      <td>Type_1</td>\n",
       "      <td>5</td>\n",
       "      <td>36</td>\n",
       "      <td>173</td>\n",
       "      <td>183</td>\n",
       "      <td>3264</td>\n",
       "      <td>2448</td>\n",
       "      <td>192</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660</td>\n",
       "      <td>Type_1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>170</td>\n",
       "      <td>179</td>\n",
       "      <td>3264</td>\n",
       "      <td>2448</td>\n",
       "      <td>192</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102</td>\n",
       "      <td>Type_1</td>\n",
       "      <td>26</td>\n",
       "      <td>47</td>\n",
       "      <td>136</td>\n",
       "      <td>160</td>\n",
       "      <td>3264</td>\n",
       "      <td>2448</td>\n",
       "      <td>192</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1226</td>\n",
       "      <td>Type_1</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>155</td>\n",
       "      <td>124</td>\n",
       "      <td>4128</td>\n",
       "      <td>3096</td>\n",
       "      <td>192</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id    type   x   y    w    h  img_shp_0_init  img_shape1_init  \\\n",
       "0       579  Type_1   1  60  190  155            4128             3096   \n",
       "1       787  Type_1   5  36  173  183            3264             2448   \n",
       "2       660  Type_1   1  33  170  179            3264             2448   \n",
       "3       102  Type_1  26  47  136  160            3264             2448   \n",
       "4      1226  Type_1   1  71  155  124            4128             3096   \n",
       "\n",
       "   img_shp_0  img_shp_1  \n",
       "0        192        256  \n",
       "1        192        256  \n",
       "2        192        256  \n",
       "3        192        256  \n",
       "4        192        256  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1480, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_files = []\n",
    "\n",
    "for type in enumerate(types):\n",
    "    all_files.append(glob(os.path.join(TRAIN_DATA, type[1], \"*.jpg\")))\n",
    "\n",
    "all_files = np.concatenate(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_dict = {}\n",
    "for item in all_files:\n",
    "    item_idx = int(item.split('/')[-1][:-4])\n",
    "    \n",
    "    match = df[df['image_id'] == item_idx]\n",
    "    \n",
    "    xd = int(match['x'].item())\n",
    "    yd = int(match['y'].item())\n",
    "    wd = int(match['w'].item())\n",
    "    hd = int(match['h'].item())\n",
    "    img_shp_0d = int(match['img_shp_0'].item())\n",
    "    img_shp_1d = int(match['img_shp_1'].item())\n",
    "    type_d = match['type'].item()\n",
    "    \n",
    "    item_dict[item_idx] = (xd, yd, wd, hd, img_shp_0d, img_shp_1d, type_d) \n",
    "    # xd:0, yd:1, wd:2, hd:3, img_0d:4, img_1d:5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 57s, sys: 12.9 s, total: 5min 10s\n",
      "Wall time: 5min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "labels = []\n",
    "segmented_images = []\n",
    "\n",
    "for item in all_files:\n",
    "    # grab image id from filename\n",
    "    item_ind = int(item.split('/')[-1][:-4])\n",
    "    img = cv2.imread(item)\n",
    "    \n",
    "    # return resizing information from dict\n",
    "    x_i, y_i, w_i, h_i, img_shp_0_i, img_shp_1_i, type_i = item_dict[item_ind]\n",
    "    \n",
    "    labels.append(type_i)\n",
    "    \n",
    "    # resize to new image dimensions\n",
    "    resized = cv2.resize(img, (img_shp_0_i, img_shp_1_i))\n",
    "\n",
    "    # crop image to segmented area\n",
    "    crop_img = resized[y_i:y_i+h_i, x_i:x_i+w_i]\n",
    "    \n",
    "    # resize image to an even smaller value for faster processing\n",
    "    desired_size = 64\n",
    "    old_size = crop_img.shape[:2]\n",
    "    \n",
    "    ratio = float(desired_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    \n",
    "    im = cv2.resize(crop_img, (new_size[1], new_size[0]))\n",
    "    \n",
    "    delta_w = desired_size - new_size[1]\n",
    "    delta_h = desired_size - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "    color = [0, 0, 0]\n",
    "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
    "    value=color)\n",
    "    \n",
    "    segmented_images.append(new_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1480, 1480)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), len(segmented_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Type_1': 249, 'Type_2': 781, 'Type_3': 450})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Building a Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Type_1', 'Type_2', 'Type_3'], \n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = le.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 249, 1: 781, 2: 450})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(segmented_images, \n",
    "                                                    labels, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=42,\n",
    "                                                   stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 64, 64, 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], 64, 64, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train, num_classes=3)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes=3)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((370, 64, 64, 3), (1110, 64, 64, 3))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((370, 3), (1110, 3))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(64,64,3),\n",
    "                     data_format=\"channels_last\"))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1110 samples, validate on 370 samples\n",
      "Epoch 1/10\n",
      "13s - loss: 1.2723 - acc: 0.4973 - val_loss: 0.9795 - val_acc: 0.5270\n",
      "Epoch 2/10\n",
      "12s - loss: 0.9929 - acc: 0.5225 - val_loss: 1.0047 - val_acc: 0.5216\n",
      "Epoch 3/10\n",
      "12s - loss: 0.9738 - acc: 0.5351 - val_loss: 1.0077 - val_acc: 0.5135\n",
      "Epoch 4/10\n",
      "12s - loss: 0.9247 - acc: 0.5495 - val_loss: 1.0196 - val_acc: 0.5270\n",
      "Epoch 5/10\n",
      "12s - loss: 0.8865 - acc: 0.5793 - val_loss: 0.9974 - val_acc: 0.4946\n",
      "Epoch 6/10\n",
      "12s - loss: 0.8601 - acc: 0.5847 - val_loss: 1.0178 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "12s - loss: 0.8373 - acc: 0.6108 - val_loss: 1.0883 - val_acc: 0.5108\n",
      "Epoch 8/10\n",
      "12s - loss: 0.7830 - acc: 0.6351 - val_loss: 1.0839 - val_acc: 0.5081\n",
      "Epoch 9/10\n",
      "12s - loss: 0.7482 - acc: 0.6640 - val_loss: 1.1345 - val_acc: 0.4973\n",
      "Epoch 10/10\n",
      "12s - loss: 0.6966 - acc: 0.7180 - val_loss: 1.1469 - val_acc: 0.4865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f282a059b38>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = conv_model()\n",
    "\n",
    "# Fit the model\n",
    "# make sure batch size is perfectly divisible\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=20,\n",
    "verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization, concatenate, Activation, AveragePooling2D, GlobalAveragePooling2D\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x): return Activation('relu')(x)\n",
    "def dropout(x, p): return Dropout(p)(x) if p else x\n",
    "def bn(x): return BatchNormalization(axis=-1)(x)\n",
    "def relu_bn(x): return relu(bn(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(x, nf, sz, wd, p):\n",
    "    x = Convolution2D(filters=nf, kernel_size=sz, strides=(sz, sz), kernel_initializer='he_uniform', padding='same', \n",
    "                          kernel_regularizer=l2(wd))(x)\n",
    "    return dropout(x,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_block(x, nf, bottleneck=False, p=None, wd=0):\n",
    "    x = relu_bn(x)\n",
    "    if bottleneck: x = relu_bn(conv(x, nf * 4, 1, wd, p))\n",
    "    return conv(x, nf, 3, wd, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_block(x, nb_layers, growth_rate, bottleneck=False, p=None, wd=0):\n",
    "    if bottleneck: nb_layers //= 2\n",
    "    for i in range(nb_layers):\n",
    "        b = conv_block(x, growth_rate, bottleneck=bottleneck, p=p, wd=wd)\n",
    "        x = concatenate([x,b], axis=-1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transition_block(x, compression=1.0, p=None, wd=0):\n",
    "    nf = int(x.get_shape().as_list()[-1] * compression)\n",
    "    x = relu_bn(x)\n",
    "    x = conv(x, nf, 1, wd, p)\n",
    "    return AveragePooling2D((2, 2), strides=(2, 2))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dense_net(nb_classes, img_input, depth=40, nb_block=3, \n",
    "     growth_rate=12, nb_filter=16, bottleneck=False, compression=1.0, p=None, wd=0, activation='softmax'):\n",
    "    \n",
    "    assert activation == 'softmax' or activation == 'sigmoid'\n",
    "    #assert (depth - 4) % nb_block == 0\n",
    "    nb_layers_per_block = int((depth - 4) / nb_block)\n",
    "    nb_layers = [nb_layers_per_block] * nb_block\n",
    "\n",
    "    x = conv(img_input, nb_filter, 3, wd, 0)\n",
    "    for i,block in enumerate(nb_layers):\n",
    "        x = dense_block(x, block, growth_rate, bottleneck=bottleneck, p=p, wd=wd)\n",
    "        if i != len(nb_layers)-1:\n",
    "            x = transition_block(x, compression=compression, p=p, wd=wd)\n",
    "\n",
    "    x = relu_bn(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    return Dense(nb_classes, activation=activation, kernel_regularizer=l2(wd))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 22, 22, 16), (None, 8, 8, 12)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-c8acd8c6c00c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimg_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m x = create_dense_net(3, img_input, depth=12, nb_filter=16, compression=0.5, \n\u001b[0;32m----> 4\u001b[0;31m                      bottleneck=True, p=0.2, wd=1e-4)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-194-74b3ed1be15b>\u001b[0m in \u001b[0;36mcreate_dense_net\u001b[0;34m(nb_classes, img_input, depth, nb_block, growth_rate, nb_filter, bottleneck, compression, p, wd, activation)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdense_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrowth_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottleneck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbottleneck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransition_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-192-ecbb56de6db0>\u001b[0m in \u001b[0;36mdense_block\u001b[0;34m(x, nb_layers, growth_rate, bottleneck, p, wd)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrowth_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottleneck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbottleneck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/layers/merge.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(inputs, axis, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mconcatenation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minputs\u001b[0m \u001b[0malongside\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \"\"\"\n\u001b[0;32m--> 508\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/layers/merge.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    275\u001b[0m                              \u001b[0;34m'inputs with matching shapes '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                              \u001b[0;34m'except for the concat axis. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                              'Got inputs shapes: %s' % (input_shape))\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 22, 22, 16), (None, 8, 8, 12)]"
     ]
    }
   ],
   "source": [
    "input_shape = (64,64,3)\n",
    "img_input = Input(shape=input_shape)\n",
    "x = create_dense_net(3, img_input, depth=12, nb_filter=16, compression=0.5, \n",
    "                     bottleneck=True, p=0.2, wd=1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
