{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import math\n",
    "from sklearn import mixture\n",
    "from sklearn.utils import shuffle\n",
    "from skimage import measure\n",
    "from glob import glob\n",
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type_1\n",
      "Type_2\n",
      "Type_3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(check_output([\"ls\", \"../train\"]).decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA = \"../train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "types = ['Type_1', 'Type_2', 'Type_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../train/Type_1/1346.jpg']\n",
      "['1346']\n",
      "['../train/Type_2/235.jpg']\n",
      "['235']\n",
      "['../train/Type_3/864.jpg']\n",
      "['864']\n",
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 9.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for type in enumerate(types):\n",
    "    type_i_files = glob(os.path.join(TRAIN_DATA, type[1], \"*.jpg\"))\n",
    "    print(type_i_files[:1])\n",
    "    type_i_ids = np.array([s[len(TRAIN_DATA)+8:-4] for s in type_i_files])\n",
    "    print(type_i_ids[:1])\n",
    "    type_ids.append(type_i_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_filename(image_id, image_type):\n",
    "    \"\"\"\n",
    "    Method to grab image file path from its id and tpye\n",
    "    \"\"\"\n",
    "    if image_type == \"Type_1\" or \\\n",
    "        image_type == \"Type_2\" or \\\n",
    "        image_type == \"Type_3\":\n",
    "        data_path = os.path.join(TRAIN_DATA, image_type)\n",
    "    else:\n",
    "        raise Exception(\"Image type {} is not recognized\".format(image_type))\n",
    "        \n",
    "    ext = 'jpg'\n",
    "    return os.path.join(data_path, \"{}.{}\".format(image_id, ext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_data(image_id, image_type):\n",
    "    \"\"\"\n",
    "    Method to get image data as np.array that specifies image id and type\n",
    "    \"\"\"\n",
    "    fname = get_filename(image_id, image_type)\n",
    "    img = cv2.imread(fname)\n",
    "    assert img is not None, \"Failed to read image: {}, {}\".format(image_id, image_type)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Crop images that may have circular frames present\n",
    "# Find the largest inscribed rectangled in the thresholded image\n",
    "# image is cropped to that rectangle\n",
    "\n",
    "def maxHist(hist):\n",
    "    maxArea = (0, 0, 0)\n",
    "    height = []\n",
    "    position = []\n",
    "    for i in range(len(hist)):\n",
    "        if (len(height) == 0):\n",
    "            if (hist[i] > 0):\n",
    "                height.append(hist[i])\n",
    "                position.append(i)\n",
    "        else: \n",
    "            if (hist[i] > height[-1]):\n",
    "                height.append(hist[i])\n",
    "                position.append(i)\n",
    "            elif (hist[i] < height[-1]):\n",
    "                while (height[-1] > hist[i]):\n",
    "                    maxHeight = height.pop()\n",
    "                    area = maxHeight * (i-position[-1])\n",
    "                    if (area > maxArea[0]):\n",
    "                        maxArea = (area, position[-1], i)\n",
    "                    last_position = position.pop()\n",
    "                    if (len(height) == 0):\n",
    "                        break\n",
    "                position.append(last_position)\n",
    "                if (len(height) == 0):\n",
    "                    height.append(hist[i])\n",
    "                elif(height[-1] < hist[i]):\n",
    "                    height.append(hist[i])\n",
    "                else:\n",
    "                    position.pop()    \n",
    "    while (len(height) > 0):\n",
    "        maxHeight = height.pop()\n",
    "        last_position = position.pop()\n",
    "        area =  maxHeight * (len(hist) - last_position)\n",
    "        if (area > maxArea[0]):\n",
    "            maxArea = (area, len(hist), last_position)\n",
    "    return maxArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxRect(img):\n",
    "    maxArea = (0, 0, 0)\n",
    "    addMat = np.zeros(img.shape)\n",
    "    for r in range(img.shape[0]):\n",
    "        if r == 0:\n",
    "            addMat[r] = img[r]\n",
    "            area = maxHist(addMat[r])\n",
    "            if area[0] > maxArea[0]:\n",
    "                maxArea = area + (r,)\n",
    "        else:\n",
    "            addMat[r] = img[r] + addMat[r-1]\n",
    "            addMat[r][img[r] == 0] *= 0\n",
    "            area = maxHist(addMat[r])\n",
    "            if area[0] > maxArea[0]:\n",
    "                maxArea = area + (r,)\n",
    "    return (int(maxArea[3]+1-maxArea[0]/abs(maxArea[1]-maxArea[2])), maxArea[2], maxArea[3], maxArea[1], maxArea[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cropCircle(img):\n",
    "    if(img.shape[0] > img.shape[1]):\n",
    "        tile_size = (int(img.shape[1]*256/img.shape[0]),256)\n",
    "    else:\n",
    "        tile_size = (256, int(img.shape[0]*256/img.shape[1]))\n",
    "\n",
    "    img = cv2.resize(img, dsize=tile_size)\n",
    "            \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY);\n",
    "    _, thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    _, contours, _ = cv2.findContours(thresh.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    main_contour = sorted(contours, key = cv2.contourArea, reverse = True)[0]\n",
    "            \n",
    "    ff = np.zeros((gray.shape[0],gray.shape[1]), 'uint8') \n",
    "    cv2.drawContours(ff, main_contour, -1, 1, 15)\n",
    "    ff_mask = np.zeros((gray.shape[0]+2,gray.shape[1]+2), 'uint8')\n",
    "    cv2.floodFill(ff, ff_mask, (int(gray.shape[1]/2), int(gray.shape[0]/2)), 1)\n",
    "    \n",
    "    rect = maxRect(ff)\n",
    "    rectangle = [min(rect[0],rect[2]), max(rect[0],rect[2]), min(rect[1],rect[3]), max(rect[1],rect[3])]\n",
    "    img_crop = img[rectangle[0]:rectangle[1], rectangle[2]:rectangle[3]]\n",
    "    cv2.rectangle(ff,(min(rect[1],rect[3]),min(rect[0],rect[2])),(max(rect[1],rect[3]),max(rect[0],rect[2])),3,2)\n",
    "    \n",
    "    return [img_crop, rectangle, tile_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For initial delineation of cervix, use two features\n",
    "    # a color: the higher the a value, the redder the pixel color\n",
    "    # R: distance of a pixel from image center -> spatial information\n",
    "    #    -> support extraction of continuous regions\n",
    "\n",
    "def Ra_space(img, Ra_ratio, a_threshold):\n",
    "    imgLab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB);\n",
    "    w = img.shape[0]\n",
    "    h = img.shape[1]\n",
    "    Ra = np.zeros((w*h, 2))\n",
    "    for i in range(w):\n",
    "        for j in range(h):\n",
    "            R = math.sqrt((w/2-i)*(w/2-i) + (h/2-j)*(h/2-j))\n",
    "            Ra[i*h+j, 0] = R\n",
    "            Ra[i*h+j, 1] = min(imgLab[i][j][1], a_threshold)\n",
    "            \n",
    "    Ra[:,0] /= max(Ra[:,0])\n",
    "    Ra[:,0] *= Ra_ratio\n",
    "    Ra[:,1] /= max(Ra[:,1])\n",
    "\n",
    "    return Ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image is separated into two clusters in the 2-D (a-R) features space\n",
    "# use of Gaussian mixture modeling\n",
    "# initialized by k-means procedure\n",
    "\n",
    "def get_and_crop_image(image_id, image_type):\n",
    "    img = get_image_data(image_id, image_type)\n",
    "    initial_shape = img.shape\n",
    "    [img, rectangle_cropCircle, tile_size] = cropCircle(img)\n",
    "    imgLab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB);\n",
    "    w = img.shape[0]\n",
    "    h = img.shape[1]\n",
    "    Ra = Ra_space(imgLab, 1.0, 150)\n",
    "    a_channel = np.reshape(Ra[:,1], (w,h))\n",
    "    \n",
    "    g = mixture.GaussianMixture(n_components = 2, covariance_type = 'diag', random_state = 0, init_params = 'kmeans')\n",
    "    image_array_sample = shuffle(Ra, random_state=0)[:1000]\n",
    "    g.fit(image_array_sample)\n",
    "    labels = g.predict(Ra)\n",
    "    labels += 1 # Add 1 to avoid labeling as 0 since regionprops ignores the 0-label.\n",
    "    \n",
    "    # The cluster that has the highest a-mean is selected.\n",
    "    labels_2D = np.reshape(labels, (w,h))\n",
    "    gg_labels_regions = measure.regionprops(labels_2D, intensity_image = a_channel)\n",
    "    gg_intensity = [prop.mean_intensity for prop in gg_labels_regions]\n",
    "    cervix_cluster = gg_intensity.index(max(gg_intensity)) + 1\n",
    "\n",
    "    mask = np.zeros((w * h,1),'uint8')\n",
    "    mask[labels==cervix_cluster] = 255\n",
    "    mask_2D = np.reshape(mask, (w,h))\n",
    "\n",
    "    cc_labels = measure.label(mask_2D, background=0)\n",
    "    regions = measure.regionprops(cc_labels)\n",
    "    areas = [prop.area for prop in regions]\n",
    "\n",
    "    regions_label = [prop.label for prop in regions]\n",
    "    largestCC_label = regions_label[areas.index(max(areas))]\n",
    "    mask_largestCC = np.zeros((w,h),'uint8')\n",
    "    mask_largestCC[cc_labels==largestCC_label] = 255\n",
    "\n",
    "    img_masked = img.copy()\n",
    "    img_masked[mask_largestCC==0] = (0,0,0)\n",
    "    img_masked_gray = cv2.cvtColor(img_masked, cv2.COLOR_RGB2GRAY);\n",
    "            \n",
    "    _,thresh_mask = cv2.threshold(img_masked_gray,0,255,0)\n",
    "            \n",
    "    kernel = np.ones((11,11), np.uint8)\n",
    "    thresh_mask = cv2.dilate(thresh_mask, kernel, iterations = 1)\n",
    "    thresh_mask = cv2.erode(thresh_mask, kernel, iterations = 2)\n",
    "    _, contours_mask, _ = cv2.findContours(thresh_mask.copy(),cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    main_contour = sorted(contours_mask, key = cv2.contourArea, reverse = True)[0]\n",
    "    cv2.drawContours(img, main_contour, -1, 255, 3)\n",
    "    \n",
    "    x,y,w,h = cv2.boundingRect(main_contour)\n",
    "    \n",
    "    rectangle = [x+rectangle_cropCircle[2],\n",
    "                 y+rectangle_cropCircle[0],\n",
    "                 w,\n",
    "                 h,\n",
    "                 initial_shape[0],\n",
    "                 initial_shape[1],\n",
    "                 tile_size[0],\n",
    "                 tile_size[1]]\n",
    "\n",
    "    return [image_id, img, rectangle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parallelize_image_cropping(image_ids):\n",
    "    out = open('rectangles.csv', \"w\")\n",
    "    out.write(\"image_id,type,x,y,w,h,img_shp_0_init,img_shape1_init,img_shp_0,img_shp_1\\n\")\n",
    "    imf_d = {}\n",
    "    p = Pool(cpu_count())\n",
    "    for type in enumerate(types):\n",
    "        print(type)\n",
    "        partial_get_and_crop = partial(get_and_crop_image, image_type = type[1])    \n",
    "        ret = p.map(partial_get_and_crop, image_ids[type[0]])\n",
    "        for i in range(len(ret)):\n",
    "            out.write(image_ids[type[0]][i])\n",
    "            out.write(',' + str(type[1]))\n",
    "            out.write(',' + str(ret[i][2][0]))\n",
    "            out.write(',' + str(ret[i][2][1]))\n",
    "            out.write(',' + str(ret[i][2][2]))\n",
    "            out.write(',' + str(ret[i][2][3]))\n",
    "            out.write(',' + str(ret[i][2][4]))\n",
    "            out.write(',' + str(ret[i][2][5]))\n",
    "            out.write(',' + str(ret[i][2][6]))\n",
    "            out.write(',' + str(ret[i][2][7]))\n",
    "            out.write('\\n')\n",
    "            img = get_image_data(image_ids[type[0]][i], type[1])\n",
    "            if(img.shape[0] > img.shape[1]):\n",
    "                tile_size = (int(img.shape[1]*256/img.shape[0]), 256)\n",
    "            else:\n",
    "                tile_size = (256, int(img.shape[0]*256/img.shape[1]))\n",
    "            img = cv2.resize(img, dsize=tile_size)\n",
    "            cv2.rectangle(img,\n",
    "                          (ret[i][2][0], ret[i][2][1]), \n",
    "                          (ret[i][2][0]+ret[i][2][2], ret[i][2][1]+ret[i][2][3]),\n",
    "                          255,\n",
    "                          2)\n",
    "            #plt.imshow(img)\n",
    "            #plt.show()\n",
    "        ret = []\n",
    "    out.close()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'Type_1')\n",
      "(1, 'Type_2')\n",
      "(2, 'Type_3')\n",
      "CPU times: user 21min 1s, sys: 53.9 s, total: 21min 55s\n",
      "Wall time: 14min 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-1:\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/ubuntu/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parallelize_image_cropping(type_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import resizing dimensions from newly created csv\n",
    "df = pd.read_csv('rectangles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "      <th>img_shp_0_init</th>\n",
       "      <th>img_shape1_init</th>\n",
       "      <th>img_shp_0</th>\n",
       "      <th>img_shp_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1346</td>\n",
       "      <td>Type_1</td>\n",
       "      <td>17</td>\n",
       "      <td>58</td>\n",
       "      <td>156</td>\n",
       "      <td>151</td>\n",
       "      <td>4160</td>\n",
       "      <td>3120</td>\n",
       "      <td>192</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>346</td>\n",
       "      <td>Type_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>190</td>\n",
       "      <td>253</td>\n",
       "      <td>4128</td>\n",
       "      <td>3096</td>\n",
       "      <td>192</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1230</td>\n",
       "      <td>Type_1</td>\n",
       "      <td>12</td>\n",
       "      <td>42</td>\n",
       "      <td>179</td>\n",
       "      <td>169</td>\n",
       "      <td>4128</td>\n",
       "      <td>3096</td>\n",
       "      <td>192</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1456</td>\n",
       "      <td>Type_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>190</td>\n",
       "      <td>253</td>\n",
       "      <td>4128</td>\n",
       "      <td>3096</td>\n",
       "      <td>192</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Type_1</td>\n",
       "      <td>18</td>\n",
       "      <td>87</td>\n",
       "      <td>144</td>\n",
       "      <td>141</td>\n",
       "      <td>4128</td>\n",
       "      <td>3096</td>\n",
       "      <td>192</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id    type   x   y    w    h  img_shp_0_init  img_shape1_init  \\\n",
       "0      1346  Type_1  17  58  156  151            4160             3120   \n",
       "1       346  Type_1   1   1  190  253            4128             3096   \n",
       "2      1230  Type_1  12  42  179  169            4128             3096   \n",
       "3      1456  Type_1   1   1  190  253            4128             3096   \n",
       "4        10  Type_1  18  87  144  141            4128             3096   \n",
       "\n",
       "   img_shp_0  img_shp_1  \n",
       "0        192        256  \n",
       "1        192        256  \n",
       "2        192        256  \n",
       "3        192        256  \n",
       "4        192        256  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1480, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all files are accounted for\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concatenate all image paths into a single list\n",
    "all_files = []\n",
    "\n",
    "for type in enumerate(types):\n",
    "    all_files.append(glob(os.path.join(TRAIN_DATA, type[1], \"*.jpg\")))\n",
    "\n",
    "all_files = np.concatenate(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary for each image that contains the relevant resizing information\n",
    "\n",
    "item_dict = {}\n",
    "for item in all_files:\n",
    "    item_idx = int(item.split('/')[-1][:-4])\n",
    "    \n",
    "    match = df[df['image_id'] == item_idx]\n",
    "    \n",
    "    xd = int(match['x'].item())\n",
    "    yd = int(match['y'].item())\n",
    "    wd = int(match['w'].item())\n",
    "    hd = int(match['h'].item())\n",
    "    img_shp_0d = int(match['img_shp_0'].item())\n",
    "    img_shp_1d = int(match['img_shp_1'].item())\n",
    "    type_d = match['type'].item()\n",
    "    \n",
    "    item_dict[item_idx] = (xd, yd, wd, hd, img_shp_0d, img_shp_1d, type_d) \n",
    "    # xd:0, yd:1, wd:2, hd:3, img_0d:4, img_1d:5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 57s, sys: 14 s, total: 5min 11s\n",
      "Wall time: 5min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "labels = []\n",
    "segmented_images = []\n",
    "\n",
    "for item in all_files:\n",
    "    # grab image id from filename\n",
    "    item_ind = int(item.split('/')[-1][:-4])\n",
    "    img = cv2.imread(item)\n",
    "    \n",
    "    # return resizing information from dict\n",
    "    x_i, y_i, w_i, h_i, img_shp_0_i, img_shp_1_i, type_i = item_dict[item_ind]\n",
    "    \n",
    "    labels.append(type_i)\n",
    "    \n",
    "    # resize to new image dimensions\n",
    "    resized = cv2.resize(img, (img_shp_0_i, img_shp_1_i))\n",
    "\n",
    "    # crop image to segmented area\n",
    "    crop_img = resized[y_i:y_i+h_i, x_i:x_i+w_i]\n",
    "    \n",
    "    # resize image to an even smaller value for faster processing\n",
    "    desired_size = 64\n",
    "    old_size = crop_img.shape[:2]\n",
    "    \n",
    "    ratio = float(desired_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    \n",
    "    im = cv2.resize(crop_img, (new_size[1], new_size[0]))\n",
    "    \n",
    "    delta_w = desired_size - new_size[1]\n",
    "    delta_h = desired_size - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "    color = [0, 0, 0]\n",
    "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
    "    value=color)\n",
    "    \n",
    "    segmented_images.append(new_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1480, 1480)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), len(segmented_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Type_1': 249, 'Type_2': 781, 'Type_3': 450})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Building a Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Type_1', 'Type_2', 'Type_3'], \n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = le.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 249, 1: 781, 2: 450})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(segmented_images, \n",
    "                                                    labels, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=42,\n",
    "                                                   stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 64, 64, 3).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 64, 64, 3).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train, num_classes=3)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes=3)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((296, 64, 64, 3), (1184, 64, 64, 3))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((296, 3), (1184, 3))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(64,64,3),\n",
    "                     data_format=\"channels_last\"))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1184 samples, validate on 296 samples\n",
      "Epoch 1/100\n",
      "23s - loss: 1.2082 - acc: 0.4789 - val_loss: 0.9989 - val_acc: 0.5270\n",
      "Epoch 2/100\n",
      "23s - loss: 1.0011 - acc: 0.5169 - val_loss: 1.0018 - val_acc: 0.5270\n",
      "Epoch 3/100\n",
      "23s - loss: 0.9987 - acc: 0.5194 - val_loss: 1.0214 - val_acc: 0.5304\n",
      "Epoch 4/100\n",
      "23s - loss: 0.9867 - acc: 0.5245 - val_loss: 1.0017 - val_acc: 0.5270\n",
      "Epoch 5/100\n",
      "23s - loss: 0.9685 - acc: 0.5160 - val_loss: 0.9659 - val_acc: 0.5203\n",
      "Epoch 6/100\n",
      "23s - loss: 0.9205 - acc: 0.5448 - val_loss: 1.0252 - val_acc: 0.4932\n",
      "Epoch 7/100\n",
      "23s - loss: 0.9109 - acc: 0.5574 - val_loss: 0.9402 - val_acc: 0.5270\n",
      "Epoch 8/100\n",
      "23s - loss: 0.8754 - acc: 0.5625 - val_loss: 0.9370 - val_acc: 0.5473\n",
      "Epoch 9/100\n",
      "23s - loss: 0.8326 - acc: 0.6022 - val_loss: 0.9869 - val_acc: 0.5405\n",
      "Epoch 10/100\n",
      "23s - loss: 0.8150 - acc: 0.6225 - val_loss: 0.9932 - val_acc: 0.5338\n",
      "Epoch 11/100\n",
      "23s - loss: 0.8026 - acc: 0.6410 - val_loss: 0.9692 - val_acc: 0.5236\n",
      "Epoch 12/100\n",
      "23s - loss: 0.7342 - acc: 0.6698 - val_loss: 0.9804 - val_acc: 0.5405\n",
      "Epoch 13/100\n",
      "23s - loss: 0.6868 - acc: 0.6993 - val_loss: 1.0160 - val_acc: 0.5777\n",
      "Epoch 14/100\n",
      "23s - loss: 0.6226 - acc: 0.7204 - val_loss: 1.1071 - val_acc: 0.5541\n",
      "Epoch 15/100\n",
      "23s - loss: 0.5875 - acc: 0.7441 - val_loss: 1.1197 - val_acc: 0.5608\n",
      "Epoch 16/100\n",
      "23s - loss: 0.5251 - acc: 0.7880 - val_loss: 1.1431 - val_acc: 0.5270\n",
      "Epoch 17/100\n",
      "23s - loss: 0.4485 - acc: 0.8235 - val_loss: 1.3042 - val_acc: 0.5811\n",
      "Epoch 18/100\n",
      "23s - loss: 0.3641 - acc: 0.8649 - val_loss: 1.2716 - val_acc: 0.5946\n",
      "Epoch 19/100\n",
      "23s - loss: 0.3328 - acc: 0.8708 - val_loss: 1.4313 - val_acc: 0.5676\n",
      "Epoch 20/100\n",
      "23s - loss: 0.3055 - acc: 0.8860 - val_loss: 1.5048 - val_acc: 0.5507\n",
      "Epoch 21/100\n",
      "23s - loss: 0.2688 - acc: 0.8936 - val_loss: 1.5668 - val_acc: 0.5405\n",
      "Epoch 22/100\n",
      "23s - loss: 0.2253 - acc: 0.9130 - val_loss: 1.6677 - val_acc: 0.5574\n",
      "Epoch 23/100\n",
      "23s - loss: 0.1995 - acc: 0.9307 - val_loss: 1.6959 - val_acc: 0.5270\n",
      "Epoch 24/100\n",
      "23s - loss: 0.1870 - acc: 0.9282 - val_loss: 1.8054 - val_acc: 0.4831\n",
      "Epoch 25/100\n",
      "23s - loss: 0.1286 - acc: 0.9510 - val_loss: 2.0363 - val_acc: 0.5169\n",
      "Epoch 26/100\n",
      "23s - loss: 0.1351 - acc: 0.9493 - val_loss: 2.1214 - val_acc: 0.5574\n",
      "Epoch 27/100\n",
      "23s - loss: 0.1165 - acc: 0.9603 - val_loss: 2.0544 - val_acc: 0.5169\n",
      "Epoch 28/100\n",
      "23s - loss: 0.1145 - acc: 0.9611 - val_loss: 2.2738 - val_acc: 0.5507\n",
      "Epoch 29/100\n",
      "23s - loss: 0.0987 - acc: 0.9662 - val_loss: 2.0910 - val_acc: 0.4696\n",
      "Epoch 30/100\n",
      "23s - loss: 0.0861 - acc: 0.9738 - val_loss: 2.2438 - val_acc: 0.5473\n",
      "Epoch 31/100\n",
      "23s - loss: 0.0820 - acc: 0.9730 - val_loss: 2.3395 - val_acc: 0.5405\n",
      "Epoch 32/100\n",
      "23s - loss: 0.0642 - acc: 0.9772 - val_loss: 2.4500 - val_acc: 0.5000\n",
      "Epoch 33/100\n",
      "23s - loss: 0.0590 - acc: 0.9840 - val_loss: 2.4781 - val_acc: 0.5034\n",
      "Epoch 34/100\n",
      "23s - loss: 0.0518 - acc: 0.9848 - val_loss: 2.4910 - val_acc: 0.5034\n",
      "Epoch 35/100\n",
      "23s - loss: 0.0507 - acc: 0.9831 - val_loss: 2.6141 - val_acc: 0.5405\n",
      "Epoch 36/100\n",
      "23s - loss: 0.0464 - acc: 0.9840 - val_loss: 2.8136 - val_acc: 0.5574\n",
      "Epoch 37/100\n",
      "23s - loss: 0.0455 - acc: 0.9856 - val_loss: 2.5785 - val_acc: 0.5034\n",
      "Epoch 38/100\n",
      "23s - loss: 0.0611 - acc: 0.9823 - val_loss: 2.4787 - val_acc: 0.5135\n",
      "Epoch 39/100\n",
      "23s - loss: 0.0637 - acc: 0.9780 - val_loss: 2.6487 - val_acc: 0.5203\n",
      "Epoch 40/100\n",
      "23s - loss: 0.0417 - acc: 0.9873 - val_loss: 2.5943 - val_acc: 0.5236\n",
      "Epoch 41/100\n",
      "23s - loss: 0.0589 - acc: 0.9823 - val_loss: 2.7508 - val_acc: 0.5372\n",
      "Epoch 42/100\n",
      "23s - loss: 0.0438 - acc: 0.9856 - val_loss: 2.7841 - val_acc: 0.5270\n",
      "Epoch 43/100\n",
      "23s - loss: 0.0263 - acc: 0.9932 - val_loss: 3.0072 - val_acc: 0.5169\n",
      "Epoch 44/100\n",
      "23s - loss: 0.0182 - acc: 0.9949 - val_loss: 3.2045 - val_acc: 0.5101\n",
      "Epoch 45/100\n",
      "23s - loss: 0.0491 - acc: 0.9831 - val_loss: 3.0613 - val_acc: 0.4932\n",
      "Epoch 46/100\n",
      "23s - loss: 0.0757 - acc: 0.9755 - val_loss: 2.7012 - val_acc: 0.4865\n",
      "Epoch 47/100\n",
      "23s - loss: 0.0433 - acc: 0.9873 - val_loss: 2.7119 - val_acc: 0.5304\n",
      "Epoch 48/100\n",
      "23s - loss: 0.0340 - acc: 0.9882 - val_loss: 2.8202 - val_acc: 0.5236\n",
      "Epoch 49/100\n",
      "23s - loss: 0.0334 - acc: 0.9932 - val_loss: 3.0695 - val_acc: 0.5304\n",
      "Epoch 50/100\n",
      "23s - loss: 0.0356 - acc: 0.9890 - val_loss: 3.1822 - val_acc: 0.5236\n",
      "Epoch 51/100\n",
      "23s - loss: 0.0553 - acc: 0.9789 - val_loss: 2.7728 - val_acc: 0.5101\n",
      "Epoch 52/100\n",
      "23s - loss: 0.0640 - acc: 0.9797 - val_loss: 3.2605 - val_acc: 0.5000\n",
      "Epoch 53/100\n",
      "23s - loss: 0.0591 - acc: 0.9831 - val_loss: 2.8733 - val_acc: 0.4899\n",
      "Epoch 54/100\n",
      "23s - loss: 0.0300 - acc: 0.9907 - val_loss: 3.0108 - val_acc: 0.5101\n",
      "Epoch 55/100\n",
      "23s - loss: 0.0257 - acc: 0.9932 - val_loss: 3.0700 - val_acc: 0.5135\n",
      "Epoch 56/100\n",
      "23s - loss: 0.0202 - acc: 0.9949 - val_loss: 3.3841 - val_acc: 0.5034\n",
      "Epoch 57/100\n",
      "23s - loss: 0.0190 - acc: 0.9941 - val_loss: 3.3844 - val_acc: 0.4831\n",
      "Epoch 58/100\n",
      "23s - loss: 0.0182 - acc: 0.9941 - val_loss: 3.4977 - val_acc: 0.5101\n",
      "Epoch 59/100\n",
      "23s - loss: 0.0165 - acc: 0.9975 - val_loss: 3.4233 - val_acc: 0.4932\n",
      "Epoch 60/100\n",
      "23s - loss: 0.0231 - acc: 0.9941 - val_loss: 3.2946 - val_acc: 0.5236\n",
      "Epoch 61/100\n",
      "23s - loss: 0.0343 - acc: 0.9899 - val_loss: 3.1596 - val_acc: 0.5101\n",
      "Epoch 62/100\n",
      "23s - loss: 0.0256 - acc: 0.9916 - val_loss: 2.9888 - val_acc: 0.5203\n",
      "Epoch 63/100\n",
      "23s - loss: 0.0125 - acc: 0.9966 - val_loss: 3.3906 - val_acc: 0.5203\n",
      "Epoch 64/100\n",
      "23s - loss: 0.0341 - acc: 0.9882 - val_loss: 3.2553 - val_acc: 0.5034\n",
      "Epoch 65/100\n",
      "23s - loss: 0.0262 - acc: 0.9916 - val_loss: 3.3283 - val_acc: 0.5405\n",
      "Epoch 66/100\n",
      "23s - loss: 0.0205 - acc: 0.9932 - val_loss: 3.3547 - val_acc: 0.5101\n",
      "Epoch 67/100\n",
      "23s - loss: 0.0333 - acc: 0.9899 - val_loss: 2.9979 - val_acc: 0.5135\n",
      "Epoch 68/100\n",
      "23s - loss: 0.0277 - acc: 0.9924 - val_loss: 3.1622 - val_acc: 0.5169\n",
      "Epoch 69/100\n",
      "23s - loss: 0.0131 - acc: 0.9958 - val_loss: 3.4906 - val_acc: 0.5236\n",
      "Epoch 70/100\n",
      "23s - loss: 0.0137 - acc: 0.9958 - val_loss: 3.4424 - val_acc: 0.5405\n",
      "Epoch 71/100\n",
      "23s - loss: 0.0339 - acc: 0.9907 - val_loss: 3.3014 - val_acc: 0.5507\n",
      "Epoch 72/100\n",
      "23s - loss: 0.0318 - acc: 0.9899 - val_loss: 3.4139 - val_acc: 0.5304\n",
      "Epoch 73/100\n",
      "23s - loss: 0.0255 - acc: 0.9932 - val_loss: 3.3576 - val_acc: 0.5101\n",
      "Epoch 74/100\n",
      "23s - loss: 0.0280 - acc: 0.9916 - val_loss: 3.2484 - val_acc: 0.5236\n",
      "Epoch 75/100\n",
      "23s - loss: 0.0152 - acc: 0.9966 - val_loss: 3.3286 - val_acc: 0.5405\n",
      "Epoch 76/100\n",
      "23s - loss: 0.0170 - acc: 0.9941 - val_loss: 3.2866 - val_acc: 0.5236\n",
      "Epoch 77/100\n",
      "23s - loss: 0.0205 - acc: 0.9949 - val_loss: 3.5840 - val_acc: 0.5642\n",
      "Epoch 78/100\n",
      "23s - loss: 0.0143 - acc: 0.9966 - val_loss: 3.3947 - val_acc: 0.5304\n",
      "Epoch 79/100\n",
      "23s - loss: 0.0149 - acc: 0.9949 - val_loss: 3.3318 - val_acc: 0.5169\n",
      "Epoch 80/100\n",
      "23s - loss: 0.0147 - acc: 0.9941 - val_loss: 3.5881 - val_acc: 0.5372\n",
      "Epoch 81/100\n",
      "23s - loss: 0.0169 - acc: 0.9941 - val_loss: 3.4958 - val_acc: 0.5135\n",
      "Epoch 82/100\n",
      "23s - loss: 0.0173 - acc: 0.9941 - val_loss: 3.8486 - val_acc: 0.5068\n",
      "Epoch 83/100\n",
      "23s - loss: 0.0144 - acc: 0.9941 - val_loss: 3.6298 - val_acc: 0.5236\n",
      "Epoch 84/100\n",
      "23s - loss: 0.0098 - acc: 0.9992 - val_loss: 3.9338 - val_acc: 0.5068\n",
      "Epoch 85/100\n",
      "23s - loss: 0.0146 - acc: 0.9958 - val_loss: 3.5295 - val_acc: 0.5169\n",
      "Epoch 86/100\n",
      "23s - loss: 0.0414 - acc: 0.9865 - val_loss: 3.4380 - val_acc: 0.5642\n",
      "Epoch 87/100\n",
      "23s - loss: 0.0838 - acc: 0.9755 - val_loss: 3.3473 - val_acc: 0.5236\n",
      "Epoch 88/100\n",
      "23s - loss: 0.0433 - acc: 0.9882 - val_loss: 3.1115 - val_acc: 0.5405\n",
      "Epoch 89/100\n",
      "23s - loss: 0.0360 - acc: 0.9882 - val_loss: 3.3730 - val_acc: 0.5135\n",
      "Epoch 90/100\n",
      "23s - loss: 0.0222 - acc: 0.9924 - val_loss: 3.7997 - val_acc: 0.5372\n",
      "Epoch 91/100\n",
      "23s - loss: 0.0195 - acc: 0.9916 - val_loss: 3.5906 - val_acc: 0.4932\n",
      "Epoch 92/100\n",
      "23s - loss: 0.0335 - acc: 0.9907 - val_loss: 3.4337 - val_acc: 0.5101\n",
      "Epoch 93/100\n",
      "23s - loss: 0.0175 - acc: 0.9916 - val_loss: 3.7036 - val_acc: 0.4899\n",
      "Epoch 94/100\n",
      "23s - loss: 0.0217 - acc: 0.9941 - val_loss: 3.5268 - val_acc: 0.5203\n",
      "Epoch 95/100\n",
      "23s - loss: 0.0096 - acc: 0.9975 - val_loss: 3.7388 - val_acc: 0.5270\n",
      "Epoch 96/100\n",
      "23s - loss: 0.0166 - acc: 0.9975 - val_loss: 3.8704 - val_acc: 0.5169\n",
      "Epoch 97/100\n",
      "23s - loss: 0.0202 - acc: 0.9958 - val_loss: 3.5971 - val_acc: 0.5101\n",
      "Epoch 98/100\n",
      "23s - loss: 0.0192 - acc: 0.9941 - val_loss: 3.5334 - val_acc: 0.5270\n",
      "Epoch 99/100\n",
      "23s - loss: 0.0123 - acc: 0.9966 - val_loss: 3.9514 - val_acc: 0.5000\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23s - loss: 0.0086 - acc: 0.9958 - val_loss: 4.0389 - val_acc: 0.5068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7effeb1e5c18>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = conv_model()\n",
    "\n",
    "# Fit the model\n",
    "# make sure batch size is perfectly divisible\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=20,\n",
    "verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alexnet_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(96, kernel_size=(3,3),\n",
    "                     strides=(2,2),\n",
    "                     activation='relu',\n",
    "                     padding='same',\n",
    "                     input_shape=(64,64,3),\n",
    "                     data_format=\"channels_last\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2),\n",
    "                           strides=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(256, (5,5), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(384, (3,3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(384, (3,3), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1184 samples, validate on 296 samples\n",
      "Epoch 1/30\n",
      "87s - loss: 4.7929 - acc: 0.4645 - val_loss: 1.0038 - val_acc: 0.5270\n",
      "Epoch 2/30\n",
      "85s - loss: 1.9839 - acc: 0.4291 - val_loss: 1.9286 - val_acc: 0.3041\n",
      "Epoch 3/30\n",
      "85s - loss: 1.8064 - acc: 0.3809 - val_loss: 1.2150 - val_acc: 0.3074\n",
      "Epoch 4/30\n",
      "85s - loss: 1.9487 - acc: 0.3868 - val_loss: 1.0147 - val_acc: 0.5270\n",
      "Epoch 5/30\n",
      "85s - loss: 1.5811 - acc: 0.4037 - val_loss: 1.3365 - val_acc: 0.3041\n",
      "Epoch 6/30\n",
      "85s - loss: 1.5708 - acc: 0.4198 - val_loss: 1.1720 - val_acc: 0.2027\n",
      "Epoch 7/30\n",
      "85s - loss: 1.6902 - acc: 0.3970 - val_loss: 2.1782 - val_acc: 0.4561\n",
      "Epoch 8/30\n",
      "85s - loss: 1.8049 - acc: 0.3564 - val_loss: 1.2037 - val_acc: 0.5101\n",
      "Epoch 9/30\n",
      "85s - loss: 1.5360 - acc: 0.4003 - val_loss: 1.2933 - val_acc: 0.4966\n",
      "Epoch 10/30\n",
      "85s - loss: 1.7071 - acc: 0.3809 - val_loss: 1.4043 - val_acc: 0.4358\n",
      "Epoch 11/30\n",
      "85s - loss: 1.5080 - acc: 0.3927 - val_loss: 1.1176 - val_acc: 0.5135\n",
      "Epoch 12/30\n",
      "85s - loss: 1.5381 - acc: 0.4257 - val_loss: 1.0712 - val_acc: 0.4054\n",
      "Epoch 13/30\n",
      "85s - loss: 1.5264 - acc: 0.4020 - val_loss: 1.4425 - val_acc: 0.4527\n",
      "Epoch 14/30\n",
      "85s - loss: 1.4329 - acc: 0.4324 - val_loss: 1.1459 - val_acc: 0.3581\n",
      "Epoch 15/30\n",
      "85s - loss: 1.4178 - acc: 0.4206 - val_loss: 1.0878 - val_acc: 0.4966\n",
      "Epoch 16/30\n",
      "85s - loss: 1.5224 - acc: 0.4020 - val_loss: 1.5416 - val_acc: 0.3412\n",
      "Epoch 17/30\n",
      "85s - loss: 1.6667 - acc: 0.3953 - val_loss: 1.0586 - val_acc: 0.3851\n",
      "Epoch 18/30\n",
      "85s - loss: 1.4962 - acc: 0.4096 - val_loss: 1.6012 - val_acc: 0.4561\n",
      "Epoch 19/30\n",
      "85s - loss: 1.4772 - acc: 0.4063 - val_loss: 1.1299 - val_acc: 0.2703\n",
      "Epoch 20/30\n",
      "85s - loss: 1.5158 - acc: 0.4046 - val_loss: 1.0247 - val_acc: 0.5270\n",
      "Epoch 21/30\n",
      "85s - loss: 1.4106 - acc: 0.4071 - val_loss: 1.8731 - val_acc: 0.4527\n",
      "Epoch 22/30\n",
      "85s - loss: 1.4623 - acc: 0.4003 - val_loss: 1.0323 - val_acc: 0.3682\n",
      "Epoch 23/30\n",
      "85s - loss: 1.4931 - acc: 0.3961 - val_loss: 1.2999 - val_acc: 0.4459\n",
      "Epoch 24/30\n",
      "85s - loss: 1.4563 - acc: 0.4037 - val_loss: 1.3181 - val_acc: 0.2534\n",
      "Epoch 25/30\n",
      "85s - loss: 1.4318 - acc: 0.3910 - val_loss: 1.2102 - val_acc: 0.3547\n",
      "Epoch 26/30\n",
      "85s - loss: 1.3941 - acc: 0.4400 - val_loss: 1.3364 - val_acc: 0.3615\n",
      "Epoch 27/30\n",
      "85s - loss: 1.3877 - acc: 0.4307 - val_loss: 1.3158 - val_acc: 0.4797\n",
      "Epoch 28/30\n",
      "85s - loss: 1.3481 - acc: 0.4443 - val_loss: 1.7126 - val_acc: 0.2635\n",
      "Epoch 29/30\n",
      "85s - loss: 1.5969 - acc: 0.4189 - val_loss: 1.9130 - val_acc: 0.3311\n",
      "Epoch 30/30\n",
      "85s - loss: 1.4430 - acc: 0.4139 - val_loss: 1.4496 - val_acc: 0.4730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7effe3032f28>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = alexnet_model()\n",
    "\n",
    "# Fit the model\n",
    "# make sure batch size is perfectly divisible\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=20,\n",
    "verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
