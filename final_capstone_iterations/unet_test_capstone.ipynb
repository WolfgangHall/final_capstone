{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from glob import glob\n",
    "from skimage.io import imread, imshow\n",
    "import piexif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using os sets the \\ for me -> windows\n",
    "# using os / -> linux\n",
    "basepath = os.path.join('..', 'train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../train/\n"
     ]
    }
   ],
   "source": [
    "print(basepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_cervix_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cervix_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for path in sorted(glob(basepath + \"*\")):\n",
    "    # escape \\\n",
    "    cervix_type = path.split(\"/\")[-1]\n",
    "    cervix_images = sorted((glob(basepath + cervix_type + \"/*\")))\n",
    "    all_cervix_images = all_cervix_images + cervix_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../train/Type_1/0.jpg',\n",
       " '../train/Type_1/10.jpg',\n",
       " '../train/Type_1/1013.jpg',\n",
       " '../train/Type_1/1014.jpg',\n",
       " '../train/Type_1/1019.jpg']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cervix_images[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_cervix_images = pd.DataFrame({'imagepath': all_cervix_images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imagepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../train/Type_1/0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../train/Type_1/10.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../train/Type_1/1013.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../train/Type_1/1014.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../train/Type_1/1019.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  imagepath\n",
       "0     ../train/Type_1/0.jpg\n",
       "1    ../train/Type_1/10.jpg\n",
       "2  ../train/Type_1/1013.jpg\n",
       "3  ../train/Type_1/1014.jpg\n",
       "4  ../train/Type_1/1019.jpg"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cervix_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_cervix_images['filetype'] = all_cervix_images.apply(lambda row: row.imagepath.split(\".\")[-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imagepath</th>\n",
       "      <th>filetype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../train/Type_1/0.jpg</td>\n",
       "      <td>jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../train/Type_1/10.jpg</td>\n",
       "      <td>jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../train/Type_1/1013.jpg</td>\n",
       "      <td>jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../train/Type_1/1014.jpg</td>\n",
       "      <td>jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../train/Type_1/1019.jpg</td>\n",
       "      <td>jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  imagepath filetype\n",
       "0     ../train/Type_1/0.jpg      jpg\n",
       "1    ../train/Type_1/10.jpg      jpg\n",
       "2  ../train/Type_1/1013.jpg      jpg\n",
       "3  ../train/Type_1/1014.jpg      jpg\n",
       "4  ../train/Type_1/1019.jpg      jpg"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cervix_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_cervix_images['type'] = all_cervix_images.apply(lambda row: row.imagepath.split(\"/\")[-2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imagepath</th>\n",
       "      <th>filetype</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../train/Type_1/0.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>Type_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../train/Type_1/10.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>Type_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../train/Type_1/1013.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>Type_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../train/Type_1/1014.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>Type_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../train/Type_1/1019.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>Type_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  imagepath filetype    type\n",
       "0     ../train/Type_1/0.jpg      jpg  Type_1\n",
       "1    ../train/Type_1/10.jpg      jpg  Type_1\n",
       "2  ../train/Type_1/1013.jpg      jpg  Type_1\n",
       "3  ../train/Type_1/1014.jpg      jpg  Type_1\n",
       "4  ../train/Type_1/1019.jpg      jpg  Type_1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cervix_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import ndimage, misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 1480 images in the whole dataset\n"
     ]
    }
   ],
   "source": [
    "print('We have a total of {} images in the whole dataset'.format(all_cervix_images.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f23136f62e8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:1297: UserWarning: findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuQAAAHwCAYAAADuC3p1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUJAgBwCAgv4/iZn7VRSiiKgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f231370a240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.set_title('Cervix Types in Dataset')\n",
    "sns.countplot(y='type',data=all_cervix_images, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percantage of each type in the data:\n",
      "Type_1: 16.8 %\n",
      "Type_2: 52.8 %\n",
      "Type_3: 30.4 %\n"
     ]
    }
   ],
   "source": [
    "print('Percantage of each type in the data:')\n",
    "for c_type in ['Type_1', 'Type_2', 'Type_3']:\n",
    "    print(c_type + ': ' + '{:.1f} %'.format(\n",
    "        len(all_cervix_images[all_cervix_images.type == c_type]) / len(all_cervix_images) * 100)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(12,8))\n",
    "\n",
    "# i = 1\n",
    "# for t in all_cervix_images['type'].unique():\n",
    "#     ax = fig.add_subplot(1,3,i)\n",
    "#     i+=1\n",
    "#     f = all_cervix_images[all_cervix_images['type'] == t]['imagepath'].values[0]\n",
    "#     plt.imshow(plt.imread(f))\n",
    "#     plt.title('sample for cervix {}'.format(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(all_cervix_images['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Type_1', 'Type_2', 'Type_3'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_cervix_images['l_type'] = le.transform(all_cervix_images['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imagepath</th>\n",
       "      <th>filetype</th>\n",
       "      <th>type</th>\n",
       "      <th>l_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../train/Type_1/0.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>Type_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../train/Type_1/10.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>Type_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../train/Type_1/1013.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>Type_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../train/Type_1/1014.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>Type_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../train/Type_1/1019.jpg</td>\n",
       "      <td>jpg</td>\n",
       "      <td>Type_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  imagepath filetype    type  l_type\n",
       "0     ../train/Type_1/0.jpg      jpg  Type_1       0\n",
       "1    ../train/Type_1/10.jpg      jpg  Type_1       0\n",
       "2  ../train/Type_1/1013.jpg      jpg  Type_1       0\n",
       "3  ../train/Type_1/1014.jpg      jpg  Type_1       0\n",
       "4  ../train/Type_1/1019.jpg      jpg  Type_1       0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cervix_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    781\n",
       "2    450\n",
       "0    249\n",
       "Name: l_type, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cervix_images['l_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dimensions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "CPU times: user 5min 33s, sys: 50 s, total: 6min 23s\n",
      "Wall time: 6min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(len(all_cervix_images)):\n",
    "    dimensions.append(imread(all_cervix_images['imagepath'].iloc[i]).shape)\n",
    "    if i % 100 == 0:\n",
    "        print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(640, 480, 3): 6,\n",
       "         (2448, 3264, 3): 29,\n",
       "         (3088, 4128, 3): 1,\n",
       "         (3096, 4128, 3): 14,\n",
       "         (3264, 2448, 3): 702,\n",
       "         (4128, 2322, 3): 17,\n",
       "         (4128, 3096, 3): 677,\n",
       "         (4160, 3120, 3): 34})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. 0\n",
      ".. 100\n",
      ".. 200\n",
      ".. 300\n",
      ".. 400\n",
      ".. 500\n",
      ".. 600\n",
      ".. 700\n",
      ".. 800\n",
      ".. 900\n",
      ".. 1000\n",
      ".. 1100\n",
      ".. 1200\n",
      ".. 1300\n",
      ".. 1400\n",
      "CPU times: user 7min 30s, sys: 17.6 s, total: 7min 47s\n",
      "Wall time: 7min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "images = []\n",
    "for i in range(len(all_cervix_images)):\n",
    "    image = ndimage.imread(all_cervix_images['imagepath'].iloc[i], mode='RGB')\n",
    "    image_resized = misc.imresize(image, (64, 64))\n",
    "    images.append(image_resized)\n",
    "    if i % 100 == 0:\n",
    "        print('..', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, all_cervix_images['l_type'], test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 64, 64, 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], 64, 64, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train, num_classes=3)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes=3)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((370, 3), (1110, 3))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'builtin_function_or_method' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-71c4c311d73c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'builtin_function_or_method' has no len()"
     ]
    }
   ],
   "source": [
    "len(y_train.flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((370, 64, 64, 3), (1110, 64, 64, 3))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center=True, \n",
    "                             featurewise_std_normalization=True)\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "#for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=10):\n",
    "#    for i in range(0, 9):\n",
    "#        plt.subplot(330 + 1 + i)\n",
    "#        plt.imshow(X_batch[i])\n",
    "        \n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    # convolutional layer, 32 feature maps, size of 3x3 ,rectifier activation, input layer\n",
    "    model.add(Conv2D(32, (3, 3), padding='valid', input_shape=(64, 64, 3),\n",
    "    activation='relu', data_format='channels_last'))\n",
    "\n",
    "    # pool size of 2x2\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # converts 2d matrix data to a vector\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # fully connected layer with 128 neurons, rectifier is used\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "\n",
    "    # output layer has 3 neurons for the 3 classes, softmax returns probability-like prediction\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1110 samples, validate on 370 samples\n",
      "Epoch 1/100\n",
      "6s - loss: 1.0171 - acc: 0.5261 - val_loss: 0.9543 - val_acc: 0.5243\n",
      "Epoch 2/100\n",
      "3s - loss: 0.9604 - acc: 0.5405 - val_loss: 0.9243 - val_acc: 0.5054\n",
      "Epoch 3/100\n",
      "3s - loss: 0.9152 - acc: 0.5613 - val_loss: 0.9547 - val_acc: 0.5351\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-ac38c7ccb210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# make sure batch size is perfectly divisible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=10,\n\u001b[0;32m----> 6\u001b[0;31m verbose=2)\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = conv_model()\n",
    "\n",
    "# Fit the model\n",
    "# make sure batch size is perfectly divisible\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=10,\n",
    "verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2s - loss: 6.8308 - acc: 0.5703 - val_loss: 7.9501 - val_acc: 0.5054\n",
      "Epoch 2/100\n",
      "2s - loss: 7.4687 - acc: 0.5342 - val_loss: 7.9351 - val_acc: 0.5054\n",
      "Epoch 3/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 4/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 5/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 6/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 7/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 8/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 9/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 10/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 11/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 12/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 13/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 14/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 15/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 16/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 17/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 18/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 19/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 20/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 21/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 22/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 23/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 24/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 25/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 26/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 27/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 28/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 29/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 30/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 31/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 32/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 33/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 34/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 35/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 36/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 37/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 38/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 39/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 40/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 41/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 42/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 43/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 44/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 45/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 46/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 47/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 48/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 49/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 50/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 51/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 52/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 53/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 54/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 55/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 56/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 57/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 58/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 59/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 60/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 61/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 62/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 63/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 64/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 65/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 66/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 67/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 68/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 69/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 70/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 71/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 72/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 73/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 74/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 75/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 76/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 77/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 78/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 79/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 80/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 81/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 82/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 83/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 84/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 85/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 86/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 87/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 88/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 89/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 90/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 91/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 92/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 93/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 94/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 95/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 96/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 97/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 98/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 99/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n",
      "Epoch 100/100\n",
      "2s - loss: 7.4637 - acc: 0.5369 - val_loss: 7.9356 - val_acc: 0.5054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6706b4c7f0>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fits the model on batches with real-time data augmentation:\n",
    "model.fit_generator(datagen.flow(X_train, y_train, batch_size=10),\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    steps_per_epoch=len(X_train) / 10, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import (Input, concatenate, core, \n",
    "                          Conv2DTranspose, UpSampling2D)\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet():\n",
    "    inputs = Input((64, 64, 3))\n",
    "    conv1 = Conv2D(32, (3,3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(32, (3,3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2,2))(conv1)\n",
    "    \n",
    "    \n",
    "    conv2 = Conv2D(64, (3,3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(64, (3,3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2,2))(conv2)\n",
    "    \n",
    "    \n",
    "    up3 = concatenate([Conv2DTranspose(32, (2,2), strides=(2,2), padding='same')(conv2), conv1], axis=3)\n",
    "    conv3 = Conv2D(32, (3,3), activation='relu', padding='same')(up3)\n",
    "    conv3 = Conv2D(32, (3,3), activation='relu', padding='same')(conv3)\n",
    "    \n",
    "\n",
    "    conv4 = Conv2D(1, (1,1), activation='softmax')(conv3)\n",
    "\n",
    "    model = Model(input=[inputs], output=[conv4])\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected conv2d_258 to have 4 dimensions, but got array with shape (1110, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-66c4734691bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# make sure batch size is perfectly divisible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=10,\n\u001b[0;32m----> 6\u001b[0;31m verbose=2)\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1520\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1523\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1380\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1383\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1384\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    130\u001b[0m                                  \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                                  \u001b[0;34m' dimensions, but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                                  str(array.shape))\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected conv2d_258 to have 4 dimensions, but got array with shape (1110, 3)"
     ]
    }
   ],
   "source": [
    "model = get_unet()\n",
    "\n",
    "# Fit the model\n",
    "# make sure batch size is perfectly divisible\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=10,\n",
    "verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
